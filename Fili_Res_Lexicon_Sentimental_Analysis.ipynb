{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andrianllmm/tagalog-stemmer.git@main\n",
        "!pip install pyspellchecker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lfitAacOmWc",
        "outputId": "5a8b25b1-9cd7-479c-89aa-0af96a6ed6d2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/andrianllmm/tagalog-stemmer.git@main\n",
            "  Cloning https://github.com/andrianllmm/tagalog-stemmer.git (to revision main) to /tmp/pip-req-build-fd1pitox\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andrianllmm/tagalog-stemmer.git /tmp/pip-req-build-fd1pitox\n",
            "  Resolved https://github.com/andrianllmm/tagalog-stemmer.git to commit b5babfd4caebf8a8f480f8adab9f1c97f42a3baa\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from tglstemmer==0.0.1) (3.9.1)\n",
            "Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from tglstemmer==0.0.1) (0.9.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.8.1->tglstemmer==0.0.1) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.8.1->tglstemmer==0.0.1) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.8.1->tglstemmer==0.0.1) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.8.1->tglstemmer==0.0.1) (4.67.1)\n",
            "Building wheels for collected packages: tglstemmer\n",
            "  Building wheel for tglstemmer (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tglstemmer: filename=tglstemmer-0.0.1-py3-none-any.whl size=146667 sha256=81c863dd4f2a7ae79608f5c501bb71263724d08b3bfbb6d20f75965864bda5e5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-efmggn5f/wheels/8c/bb/43/2bad866e363692fb17ed779ce8517901b301c705dc7a438b33\n",
            "Successfully built tglstemmer\n",
            "Installing collected packages: tglstemmer\n",
            "Successfully installed tglstemmer-0.0.1\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.2-py3-none-any.whl.metadata (9.4 kB)\n",
            "Downloading pyspellchecker-0.8.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAlihM2oz9Ht",
        "outputId": "4a7bcbaa-d88a-49b0-ea89-fa9fb28721dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive:    18 (50.0%)\n",
            "Negative:     9 (25.0%)\n",
            "Neutral:      9 (25.0%)\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import re\n",
        "import operator\n",
        "import string\n",
        "from tglstemmer import stemmer\n",
        "from spellchecker import SpellChecker\n",
        "spell = SpellChecker()\n",
        "\n",
        "mispellings = dict()\n",
        "with open(\"mispelling.csv\") as h:\n",
        "  reader = csv.reader(h)\n",
        "  for row in reader:\n",
        "    mispellings[row[0]] = row[1]\n",
        "\n",
        "tweets = []\n",
        "\n",
        "#clean up\n",
        "def remove_non_ascii(a_str):\n",
        "  ascii_chars = set(string.printable)\n",
        "\n",
        "  return \"\".join(\n",
        "      filter(lambda x: x in ascii_chars, a_str)\n",
        "  )\n",
        "\n",
        "def replace_mispellings(sentence):\n",
        "  wordlist = sentence.split()\n",
        "  for word in wordlist:\n",
        "    index = wordlist.index(word)\n",
        "    if word in mispellings:\n",
        "      word = mispellings.get(word)\n",
        "    wordlist[index] = word\n",
        "  sentence = ' '.join(wordlist)\n",
        "  return sentence\n",
        "\n",
        "def remove_punctuation(sentence):\n",
        "  punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "  no_punct = \"\"\n",
        "  for char in sentence:\n",
        "    if char not in punctuations:\n",
        "        no_punct = no_punct + char\n",
        "  return no_punct\n",
        "\n",
        "def correct_mispellings(sentence):\n",
        "    wordlist = sentence.split()\n",
        "    new_sentence = []\n",
        "    for word in wordlist:\n",
        "        corrected_word = spell.correction(word)\n",
        "        if corrected_word is None:\n",
        "            corrected_word = word\n",
        "        new_sentence.append(corrected_word)\n",
        "    return \" \".join(new_sentence)\n",
        "\n",
        "#importing list of tweets\n",
        "with open(\"newtwitter.csv\") as f:\n",
        "  reader = csv.reader(f)\n",
        "  next(reader)\n",
        "\n",
        "  for row in reader:\n",
        "    tweet = dict()\n",
        "    tweet['orig'] = row[0]\n",
        "\n",
        "    if re.match(r'^RT.*', tweet['orig']):\n",
        "      continue\n",
        "    tweet['clean'] = remove_punctuation(tweet['orig'])\n",
        "    tweet['clean'] = correct_mispellings(tweet['clean'])\n",
        "    tweet['clean'] = replace_mispellings(tweet['clean'])\n",
        "    tweet['clean'] = remove_non_ascii(tweet['clean'])\n",
        "    tweet['clean'] = tweet['clean'].lower()\n",
        "    tweets.append(tweet)\n",
        "\n",
        "lexicon_eng = dict()\n",
        "lexicon_fil = dict()\n",
        "\n",
        "#importing english lexicon\n",
        "with open(\"lexicon_eng.csv\") as f:\n",
        "  reader = csv.reader(f)\n",
        "  for row in reader:\n",
        "    lexicon_eng[row[0]] = int(row[1])\n",
        "\n",
        "#importing filipino lexicon\n",
        "with open(\"lexicon_fil.csv\") as g:\n",
        "  reader = csv.reader(g)\n",
        "  for row in reader:\n",
        "    lexicon_fil[row[0]] = float(row[1])\n",
        "\n",
        "#scoring tweets based on lexicon\n",
        "for tweet in tweets:\n",
        "  score = 0\n",
        "  for word in tweet[\"clean\"].split():\n",
        "    if word in lexicon_eng:\n",
        "      score = score + lexicon_eng[word]\n",
        "    else:\n",
        "      #if not an english word, then assume it is a filipino and stem to be able to score\n",
        "      stemword = stemmer.get_stem(word)\n",
        "      if word in lexicon_fil:\n",
        "        score = score + lexicon_fil[word]\n",
        "      else:\n",
        "        score = score\n",
        "\n",
        "  #categorizing if tweet is positive or negative\n",
        "  tweet['score'] = score\n",
        "  if (score > 0):\n",
        "    tweet['sentiment'] = 'positive'\n",
        "  elif (score < 0):\n",
        "      tweet['sentiment'] = 'negative'\n",
        "  else:\n",
        "      tweet['sentiment'] = 'neutral'\n",
        "\n",
        "#printing results\n",
        "total = float(len(tweets))\n",
        "num_pos = sum([1 for t in tweets if t['sentiment'] == 'positive'])\n",
        "num_neg = sum([1 for t in tweets if t['sentiment'] == 'negative'])\n",
        "num_neu = sum([1 for t in tweets if t['sentiment'] == 'neutral'])\n",
        "print (\"Positive: %5d (%.1f%%)\" % (num_pos, 100.0 * (num_pos/total)))\n",
        "print (\"Negative: %5d (%.1f%%)\" % (num_neg, 100.0 * (num_neg/total)))\n",
        "print (\"Neutral:  %5d (%.1f%%)\" % (num_neu, 100.0 * (num_neu/total)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spellchecker import SpellChecker\n",
        "spell = SpellChecker()\n",
        "\n",
        "# find those words that may be misspelled\n",
        "sentence = \"cmputr watr study wrte\"\n",
        "\n",
        "def correct_mispellings(sentence):\n",
        "    wordlist = sentence.split()\n",
        "    new_sentence = []\n",
        "    for word in wordlist:\n",
        "        corrected_word = spell.correction(word)\n",
        "        if corrected_word is None:\n",
        "            corrected_word = word  # fallback to original word if correction is None\n",
        "        new_sentence.append(corrected_word)\n",
        "    return \" \".join(new_sentence)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(correct_mispellings(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCrsO_vbDYRt",
        "outputId": "b25a280c-4b6a-4b1f-9d9f-91a15c54c8a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "computer water study write\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}